{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import pprint\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import *\n",
    "\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ConvBnAct(tf.keras.Model):\n",
    "  \n",
    "    def __init__(self, n_filters=64, kernel=(2,2), strides=(1,1), padding='valid'):\n",
    "        self.filters = n_filters\n",
    "        self.kernel = kernel\n",
    "        self.strides = strides\n",
    "        self.activation = tf.nn.relu\n",
    "        super(ConvBnAct, self).__init__(name='')\n",
    "        \n",
    "        #print('padding', padding)\n",
    "        self.conv_ =  Conv2D(filters=self.filters,\n",
    "                      kernel_size = self.kernel,\n",
    "                      strides = self.strides, name='Conv2D', \n",
    "                      padding=padding)\n",
    "\n",
    "        self.batch_norm = BatchNormalization(name='BatchNorm')\n",
    "\n",
    "        self.activation = Activation(self.activation, name='RELU')\n",
    "    \n",
    "    def call(self, x, training=False):\n",
    "        x = self.conv_(x) \n",
    "        x = self.batch_norm(x)\n",
    "\n",
    "        x = self.activation(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['conv_bn_act/Conv2D/kernel:0', 'conv_bn_act/Conv2D/bias:0', 'conv_bn_act/BatchNorm/gamma:0', 'conv_bn_act/BatchNorm/beta:0', 'conv_bn_act/BatchNorm/moving_mean:0', 'conv_bn_act/BatchNorm/moving_variance:0']\n"
     ]
    }
   ],
   "source": [
    "norm = tf.random_normal([10, 256, 256, 64], mean=-1, stddev=4)\n",
    "\n",
    "block = ConvBnAct()\n",
    "block(norm)\n",
    "print([x.name for x in block.variables])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvAct(tf.keras.Model):\n",
    "    def __init__(self, n_filters, kernel=(1,1), activation = tf.nn.relu, pooling=False):\n",
    "      super(ConvAct, self).__init__(name='')\n",
    "\n",
    "      self.pooling = pooling\n",
    "      self.kernel = kernel\n",
    "      self.n_filters = n_filters\n",
    "      self.activation = activation\n",
    "      self.pooling = pooling\n",
    "\n",
    "\n",
    "      self.poolingLayer = AveragePooling2D(pool_size=(1,1), padding='same', name='AveragePool')\n",
    "      self.convLayer = Conv2D(filters = self.n_filters,\n",
    "                         kernel_size = self.kernel,\n",
    "                         strides=1, name='Conv2D')\n",
    "\n",
    "      self.activation = Activation(activation, name='ActivationRelu')\n",
    "\n",
    "    def call(self, x, training = False):\n",
    "\n",
    "      if self.pooling:\n",
    "        x = self.poolingLayer(x)\n",
    "\n",
    "      x = self.convLayer(x)\n",
    "      x = self.activation(x)\n",
    "\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['conv_act/Conv2D/kernel:0', 'conv_act/Conv2D/bias:0']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "norm = tf.random_normal([10, 127, 127, 64], mean=-1, stddev=4)\n",
    "\n",
    "block = ConvAct(n_filters=64, pooling=True)\n",
    "block(norm)\n",
    "\n",
    "print([x.name for x in block.variables])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionRefinmentModule(tf.keras.Model):\n",
    "  def __init__(self, n_filters):\n",
    "    super(AttentionRefinmentModule, self).__init__(name='')\n",
    "\n",
    "    self.filters = n_filters\n",
    "    \n",
    "    self.poolingLayer = AveragePooling2D(pool_size = (1,1), padding='same', name='AveragePool')\n",
    "    self.conv_bn_act = ConvBnAct(kernel = (1,1), n_filters = self.filters)\n",
    "    \n",
    "    \n",
    "  def call(self, inputs, training = False):\n",
    "    x = self.poolingLayer(inputs)\n",
    "    x = self.conv_bn_act(x)\n",
    "    \n",
    "    print(x.shape)\n",
    "    print(inputs.shape)\n",
    "    return tf.math.multiply(inputs,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 256, 256, 64)\n",
      "(10, 256, 256, 64)\n",
      "['attention_refinment_module/conv_bn_act_1/Conv2D/kernel:0', 'attention_refinment_module/conv_bn_act_1/Conv2D/bias:0', 'attention_refinment_module/conv_bn_act_1/BatchNorm/gamma:0', 'attention_refinment_module/conv_bn_act_1/BatchNorm/beta:0', 'attention_refinment_module/conv_bn_act_1/BatchNorm/moving_mean:0', 'attention_refinment_module/conv_bn_act_1/BatchNorm/moving_variance:0']\n"
     ]
    }
   ],
   "source": [
    "norm = tf.random_normal([10, 256, 256, 64], mean=-1, stddev=4)\n",
    "\n",
    "block = AttentionRefinmentModule(n_filters=64)\n",
    "block(norm)\n",
    "\n",
    "print([x.name for x in block.variables])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FeatureFusionModule(tf.keras.Model):\n",
    "  def __init__(self, n_filters):\n",
    "    super(FeatureFusionModule, self).__init__(name='')\n",
    "    self.conv_bn_act = ConvBnAct(n_filters=n_filters, kernel=(3, 3), padding='same')\n",
    "    self.conv_act1 = ConvAct(n_filters=n_filters, pooling=True)\n",
    "    self.conv_act2 = ConvAct(n_filters=n_filters, pooling=False, activation = tf.nn.sigmoid)\n",
    "    activation = tf.nn.sigmoid\n",
    "    \n",
    "   \n",
    "  def call(self, input_f, input_s, training=True):\n",
    "    \n",
    "    print(input_f.shape, input_s.shape)\n",
    "    concate = Concatenate(axis=-1)([input_f, input_s])\n",
    "    \n",
    "    print(concate.shape)\n",
    "    branch0 = self.conv_bn_act(concate)\n",
    "    print(branch0.shape)\n",
    "    branch_1 = self.conv_act1(branch0)\n",
    "    \n",
    "    branch_1 = self.conv_act2(branch_1)\n",
    "    \n",
    "    x = multiply([branch0, branch_1])\n",
    "    return  Add()([branch0, x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 256, 256, 64) (10, 256, 256, 64)\n",
      "(10, 256, 256, 128)\n",
      "(10, 256, 256, 64)\n",
      "['feature_fusion_module/conv_bn_act_2/Conv2D/kernel:0', 'feature_fusion_module/conv_bn_act_2/Conv2D/bias:0', 'feature_fusion_module/conv_bn_act_2/BatchNorm/gamma:0', 'feature_fusion_module/conv_bn_act_2/BatchNorm/beta:0', 'feature_fusion_module/conv_act_1/Conv2D/kernel:0', 'feature_fusion_module/conv_act_1/Conv2D/bias:0', 'feature_fusion_module/conv_act_2/Conv2D/kernel:0', 'feature_fusion_module/conv_act_2/Conv2D/bias:0', 'feature_fusion_module/conv_bn_act_2/BatchNorm/moving_mean:0', 'feature_fusion_module/conv_bn_act_2/BatchNorm/moving_variance:0']\n",
      "(10, 64)\n"
     ]
    }
   ],
   "source": [
    "norm1 = tf.random_normal([10, 256, 256, 64], mean=-1, stddev=4)\n",
    "norm2 = tf.random_normal([10, 256, 256, 64], mean=-1, stddev=4)\n",
    "\n",
    "block = FeatureFusionModule(n_filters=64)\n",
    "block(norm1, norm2)\n",
    "\n",
    "print([x.name for x in block.variables])\n",
    "\n",
    "norm1 = tf.random_normal([10, 256, 256, 64], mean=-1, stddev=4)\n",
    "print(GlobalMaxPooling2D()(norm1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ContextPath(tf.keras.Model):\n",
    "  def __init__(self):\n",
    "    super(ContextPath, self).__init__(name='')\n",
    "    \n",
    "    self.globalmax = GlobalAveragePooling2D()\n",
    "    self.upsample_1 = UpSampling2D(size=(2, 2))\n",
    "    self.upsample_2 = UpSampling2D(size=(2, 2))\n",
    "    self.upsample_3 = UpSampling2D(size=(2, 2))\n",
    "    self.upsample_4 = UpSampling2D(size=(2, 2))\n",
    "    self.arm1 = AttentionRefinmentModule(n_filters=1024)\n",
    "    self.arm2 = AttentionRefinmentModule(n_filters=2048)\n",
    "\n",
    "  def call(self, layer_13, layer_14):\n",
    "    globalmax = self.globalmax(layer_14)\n",
    "    upsample1 = self.upsample_1(layer_14)\n",
    "    first_ = Add()([globalmax, upsample1])\n",
    "    \n",
    "    seg1 = self.arm1(layer_13)\n",
    "    seg2 = self.arm2(layer_14)\n",
    "    \n",
    "    up_3 = self.upsample_2(seg1)\n",
    "    up_4 = self.upsample_3(seg2)\n",
    "\n",
    "    \n",
    "    cnc = Concatenate(axis=-1)([up_3, up_4])\n",
    "    cnc = Concatenate(axis=-1)([cnc, first_])\n",
    "\n",
    "\n",
    "    context_features = self.upsample_4(cnc)\n",
    "    return context_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 7, 7, 1024)\n",
      "(10, 7, 7, 1024)\n",
      "(10, 7, 7, 2048)\n",
      "(10, 7, 7, 2048)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'context_path/up_sampling2d_3/ResizeNearestNeighbor:0' shape=(10, 28, 28, 5120) dtype=float32>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "layer_13 = tf.random_normal([10, 7, 7, 1024], mean=-1, stddev=4)\n",
    "layer_14 = tf.random_normal([10, 7, 7, 2048], mean=-1, stddev=4)\n",
    "\n",
    "block = ContextPath()\n",
    "block(layer_13,layer_14)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinalModel(tf.keras.Model):\n",
    "  def __init__(self):\n",
    "    super(FinalModel, self).__init__(name='')\n",
    "    self.context_path = ContextPath()\n",
    "    self.batch1 = ConvBnAct(32, strides=2)\n",
    "    self.batch2 = ConvBnAct(64, strides=2)\n",
    "    self.batch3 = ConvBnAct(156, strides=2)\n",
    "    self.upsample = UpSampling2D(size=(8, 8))\n",
    "    self.feature_fusion = FeatureFusionModule(32)\n",
    "    \n",
    "  def call(self, x, layer_13, layer_14): \n",
    "    \n",
    "    #spatial path\n",
    "    x = self.batch1(x)\n",
    "    x = self.batch2(x)\n",
    "    x = self.batch3(x)\n",
    "    \n",
    "    #context path\n",
    "    cp = self.context_path(layer_13, layer_14)\n",
    "    \n",
    "    #concatenate\n",
    "    print(cp.shape, x.shape, 'context')\n",
    "    \n",
    "    #upsample\n",
    "    print('fusion')\n",
    "    fusion = self.feature_fusion(cp, x, 32)\n",
    "    print('fusion')\n",
    "    ans = self.upsample(fusion)\n",
    "    \n",
    "    \n",
    "    return ans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.applications import *\n",
    "\n",
    "Xception_model = Xception(weights='imagenet',input_shape= (224,224,3), include_top=False)\n",
    "\n",
    "inputs = Input(shape=(224,224,3))\n",
    "x = Lambda(lambda image: tf.image.resize_images(image, (224, 224)))(inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 7, 7, 1024)\n",
      "(?, 7, 7, 2048)\n"
     ]
    }
   ],
   "source": [
    "layer_13 = Xception_model.get_layer('block13_pool').output\n",
    "layer_14 = Xception_model.output\n",
    "\n",
    "print(layer_13.shape)\n",
    "print(layer_14.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 16x Down\n",
    "layer_13 = Xception_model.get_layer('block13_pool').output\n",
    "layer_14 = Xception_model.output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 7, 7, 1024)\n",
      "(10, 7, 7, 1024)\n",
      "(10, 7, 7, 2048)\n",
      "(10, 7, 7, 2048)\n",
      "(10, 28, 28, 5120) (10, 28, 28, 156) context\n",
      "fusion\n",
      "(10, 28, 28, 5120) (10, 28, 28, 156)\n",
      "(10, 28, 28, 5276)\n",
      "(10, 28, 28, 32)\n",
      "fusion\n",
      "fin (10, 224, 224, 32)\n"
     ]
    }
   ],
   "source": [
    "# for layer in model.layers[:5]:\n",
    "#     layer.trainable = False\n",
    "input_layer = tf.random_normal([10, 224, 224, 3], mean=-1, stddev=4)\n",
    "\n",
    "layer_13 = tf.random_normal([10, 7, 7, 1024], mean=-1, stddev=4)\n",
    "layer_14 = tf.random_normal([10, 7, 7, 2048], mean=-1, stddev=4)\n",
    "\n",
    "block = FinalModel()\n",
    "t = block(input_layer, layer_13, layer_14)\n",
    "\n",
    "print('fin',t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.xception import Xception,preprocess_input\n",
    "from keras.utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kiko-PC\\Documents\\Image-Segmentation\\data\\train\\\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import numpy as np \n",
    "from keras.preprocessing import image\n",
    "import cv2\n",
    "\n",
    "PATH = os.getcwd()\n",
    "train_path = PATH+'\\data\\\\train\\\\'\n",
    "trainy_path = PATH+'\\data\\\\train_labels\\\\'\n",
    "\n",
    "print(train_path)\n",
    "\n",
    "train_batch = os.listdir(train_path)\n",
    "trainy_batch = os.listdir(trainy_path)\n",
    "\n",
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "\n",
    "# if data are in form of images\n",
    "for sample in train_batch:\n",
    "    img_path = train_path+sample\n",
    "    x = cv2.imread(img_path)\n",
    "    # preprocessing if required\n",
    "    x_train.append(x)\n",
    "\n",
    "\n",
    "for sample in trainy_batch:\n",
    "    img_path = trainy_path+sample\n",
    "    x = cv2.imread(img_path)\n",
    "    # preprocessing if required\n",
    "    y_train.append(x)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_label_info(csv_path):\n",
    "    \"\"\"\n",
    "    Retrieve the class names and label values for the selected dataset.\n",
    "    Must be in CSV format!\n",
    "\n",
    "    # Arguments\n",
    "        csv_path: The file path of the class dictionairy\n",
    "        \n",
    "    # Returns\n",
    "        Two lists: one for the class names and the other for the label values\n",
    "    \"\"\"\n",
    "    filename, file_extension = os.path.splitext(csv_path)\n",
    "    if not file_extension == \".csv\":\n",
    "        return ValueError(\"File is not a CSV!\")\n",
    "\n",
    "    class_names = []\n",
    "    label_values = []\n",
    "    with open(csv_path, 'r') as csvfile:\n",
    "        file_reader = csv.reader(csvfile, delimiter=',')\n",
    "        header = next(file_reader)\n",
    "        for row in file_reader:\n",
    "            class_names.append(row[0])\n",
    "            label_values.append([int(row[1]), int(row[2]), int(row[3])])\n",
    "        # print(class_dict)\n",
    "    return class_names, label_values\n",
    "\n",
    "def one_hot_it(label, label_values):\n",
    "    \"\"\"\n",
    "    Convert a segmentation image label array to one-hot format\n",
    "    by replacing each pixel value with a vector of length num_classes\n",
    "\n",
    "    # Arguments\n",
    "        label: The 2D array segmentation image label\n",
    "        label_values\n",
    "        \n",
    "    # Returns\n",
    "        A 2D array with the same width and hieght as the input, but\n",
    "        with a depth size of num_classes\n",
    "    \"\"\"\n",
    "    # st = time.time()\n",
    "    # w = label.shape[0]\n",
    "    # h = label.shape[1]\n",
    "    # num_classes = len(class_dict)\n",
    "    # x = np.zeros([w,h,num_classes])\n",
    "    # unique_labels = sortedlist((class_dict.values()))\n",
    "    # for i in range(0, w):\n",
    "    #     for j in range(0, h):\n",
    "    #         index = unique_labels.index(list(label[i][j][:]))\n",
    "    #         x[i,j,index]=1\n",
    "    # print(\"Time 1 = \", time.time() - st)\n",
    "\n",
    "    # st = time.time()\n",
    "    # https://stackoverflow.com/questions/46903885/map-rgb-semantic-maps-to-one-hot-encodings-and-vice-versa-in-tensorflow\n",
    "    # https://stackoverflow.com/questions/14859458/how-to-check-if-all-values-in-the-columns-of-a-numpy-matrix-are-the-same\n",
    "    semantic_map = []\n",
    "    for colour in label_values:\n",
    "        # colour_map = np.full((label.shape[0], label.shape[1], label.shape[2]), colour, dtype=int)\n",
    "        equality = np.equal(label, colour)\n",
    "        class_map = np.all(equality, axis = -1)\n",
    "        semantic_map.append(class_map)\n",
    "    semantic_map = np.stack(semantic_map, axis=-1)\n",
    "    # print(\"Time 2 = \", time.time() - st)\n",
    "\n",
    "    return semantic_map\n",
    "    \n",
    "def reverse_one_hot(image):\n",
    "    \"\"\"\n",
    "    Transform a 2D array in one-hot format (depth is num_classes),\n",
    "    to a 2D array with only 1 channel, where each pixel value is\n",
    "    the classified class key.\n",
    "\n",
    "    # Arguments\n",
    "        image: The one-hot format image \n",
    "        \n",
    "    # Returns\n",
    "        A 2D array with the same width and hieght as the input, but\n",
    "        with a depth size of 1, where each pixel value is the classified \n",
    "        class key.\n",
    "    \"\"\"\n",
    "    # w = image.shape[0]\n",
    "    # h = image.shape[1]\n",
    "    # x = np.zeros([w,h,1])\n",
    "\n",
    "    # for i in range(0, w):\n",
    "    #     for j in range(0, h):\n",
    "    #         index, value = max(enumerate(image[i, j, :]), key=operator.itemgetter(1))\n",
    "    #         x[i, j] = index\n",
    "\n",
    "    x = np.argmax(image, axis = -1)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "class_names_list, label_values =get_label_info(\"data/class_dict.csv\")\n",
    "class_names_string = \"\"\n",
    "for class_name in class_names_list:\n",
    "    if not class_name == class_names_list[-1]:\n",
    "        class_names_string = class_names_string + class_name + \", \"\n",
    "    else:\n",
    "        class_names_string = class_names_string + class_name\n",
    "\n",
    "num_classes = len(label_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "print(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_image = np.float32(one_hot_it(y_train, label_values=label_values))\n",
    "print(output_image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
